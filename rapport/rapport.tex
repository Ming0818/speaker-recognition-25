\documentclass{article}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[cyr]{aeguill}
\usepackage{xspace}
\usepackage{amsmath}
\usepackage[english,francais]{babel}
\usepackage{url}
\usepackage{hyperref}
\usepackage{tikz}
%%%<
%\usepackage{verbatim}
%\usepackage[active,tightpage]{preview}
%\PreviewEnvironment{center}
%\setlength\PreviewBorder{10pt}%
%%%%>
\usetikzlibrary{arrows}
\usetikzlibrary{automata}
\usetikzlibrary{positioning}
\let\urlorig\url
\renewcommand{\url}[1]{%
  \begin{otherlanguage}{english}\urlorig{#1}\end{otherlanguage}%
}
\renewcommand{\contentsname}{Sommaire} % si tableofcontents au début
\newcommand{\Numero}{\No}
\newcommand{\numero}{\no}
%\newcommand{\fup}[1]{\up{#1}}
%%% N'oubliez pas les espaces devant les doubles ponctuations
\NoAutoSpaceBeforeFDP

\title{Rapport CR12:\\
Reconnaissance de locuteur}
\author{Antoine Plet\\
Thomas Sibut-Pinote}
\date{Janvier 2014}

\begin{document}

\maketitle

\section{Introduction}
Le but de ce projet était d'implémenter un programme de reconnaissance de locuteur, c'est-à-dire, à partir de données d'entraînement, d'identifier si une personne qui parle est l'un des sujets entraînés ou non.

Nous avons essayé de retrouver les résultats du papier "Kernel Based Text-Independent Speaker Verification" de Mariéthoz, Grandvalet et Bengio (cf. \cite{Bengio}).

Nous avons commencé par coder un svm(\ref{svm}).

Puis nous avons codé le kernel perceptron pour pouvoir tester plusieurs noyaux, l'objectif étant d'arriver au noyau de Fisher(\ref{fisher}). Nous avons aussi utilisé un algorithme d'optimisation quadratique de Matlab pour comparer les résultats avec notre implémentation du kernel perceptron.

Nous avons enfin fait diverses validations (\ref{validation}) sur des données générées (\ref{donnees}) par nous-mêmes.

\section{Particularités d'implémentation}
\label{implémentation}
Nous avons travaillé sur un dépôt github (\url{https://github.com/tomsib2001/speaker-recognition}).

Nous avons utilisé le langage Python qui nous était le plus familier. Cependant, la très utile bibliothèque Voicebox étant écrite en Matlab/Octave, nous avons dû interfacer Python avec Octave (\texttt{octaveIO.py}) et Matlab pour la partie d'optimisation quadratique.

%% parler ici des graphes en GNUplot s'ils existent...

\section{Répartition du travail}
\label{répartition}
\subsection{Antoine}
\begin{itemize}
\item Interface Python/matlab
\item Test du SGD sur les données réelles
\item Passage des GMMs en Matlab\footnote{qui ne marchaient pas très bien, ne terminaient jamais sur un de nos ordinateurs et peu souvent sur l'autre} aux GMMs en Python (cf \ref{fisher})
\item Test du kernel perceptron sur les données réelles
\end{itemize}

\subsection{Thomas}
\begin{itemize}
\item Fonctions de choix des ensembles d'entraînement et d'évaluation
\item Fonctions d'affichage de graphes (\texttt{plot.py})
\item Script de récupération et nettoyage des données réelles dans le système de fichiers
\end{itemize}

\subsection{Ensemble}
\begin{itemize}
\item Écriture du svm
\item Génération de données pour tests
\item Interface de visualisation des données test en dimension 2
\item Kernel Perceptron
\item Récupération des données en .wav; interfaçage Python/octave
\item Écriture du noyau de Fisher
\item Programmation quadratique
\end{itemize}


\section{Choix des données}
Nous avons pu disposer de 5 enregistrements de personnes lisant tous la même série de 12 phrases.

Nous nous sommes de plus enregistrés tous les deux.

Enfin, nous avons pris un enregistrement d'une personnalité politique ainsi que celui d'un imitateur, pour voir à quel point il arrivait à se faire passer pour le premier. Les résultats sont en \ref{imitateur}.
\section{Représentation des données}
\label{representation}
Conformément à ce que propose l'article, nous avons choisi de transformer les données en extrayant des caractéristiques discriminantes appelées MFCCs (Mel-Frequency Cepstral Coefficients), dont les premières sont des coefficients d'une transformée discrète de cosinus (DCT), et les suivantes sont leur dérivée puis diverses fonctions du signal.

Pour cela, nous avons utilisé la librairie Voicebox (\url{http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html}).
\section{SVM}
\label{svm}
La problématique que l'on se propose de résoudre est de trouver un hyperplan séparateur entre les deux classes de données. Cette technique s'appelle un séparateur à vaste marge et on peut résoudre ce problème avec plusieurs algorithmes.
\subsection{SGD}
\label{sgd}
Nous avons commencé par programmer utilisant la technique de la Descente de Gradient Stochastique en considérant le problème:

\begin{equation}
\label{eq:primalSVM}
  \left\{
    \begin{array} {lll}
      \displaystyle\min_{w,b,\xi_i} & \frac{1}{2} \|w\|^2 + C\displaystyle\sum\limits_{i=1}^{n}\xi_i&\\
      \mbox{avec} &y_i(w^\top  x_i+b) \geq 1 - \xi_i,\quad & i=1,\dots ,n  \\
      \mbox{et} &\xi_i \geq 0 , &i=1,\dots ,n  \\
    \end{array}
  \right.
\end{equation}

On va appliquer la technique de la descente stochastique de gradient vue en cours, à la fonction convexe
\[ f_{obj}(w) := \frac{1}{2} \|w\|^2 + C\displaystyle\sum\limits_{i=1}^{n}l_i(w) \]
où $l_i(w) = \max (0, 1 - y_i (w \cdot x_i + b)) = \mathrm{hinge}(y_i (w \cdot x_i + b))$ est la fonction de coût.
On calcule un subgradient de $\mathrm{hinge}(x) = max(0,1-x)$:
\[ \partial \mathrm{hinge}(x)(h) = \begin{cases} 0 &\mbox{si } x>1 \\
-\frac{h}{2} & \mbox{si } x=1\\
-h & \mbox{si } x<1\\
 \end{cases}\]
 
 soit 
\[ \nabla \mathrm{hinge}(x) = \begin{cases} 0 &\mbox{si } x>1 \\
-\frac{1}{2} & \mbox{si } x=1\\
-1 & \mbox{si } x<1\\
 \end{cases}\]
On écrit en fait $f_{obj}$ comme une fonction de $\tilde{w} = (w,b)$ (car $b$ est un paramètre à trouver).
On a $l_i (\tilde{w}) = max(0, 1 - y_i \langle \tilde{w} , (x,1) \rangle)$ de sorte que
\[ \nabla l_i(\tilde{w}) = \begin{cases} 0 &\mbox{si } \langle \tilde{w}, (x,1) \rangle >1 \\
-\frac{y_i}{2} (x,1) & \mbox{si } \langle \tilde{w}, (x,1) \rangle=1\\
-y_i (x,1) & \mbox{si } \langle \tilde{w}, (x,1) \rangle<1\\
 \end{cases}\]
 et enfin
 \[ \nabla f_{obj}(w,b) = w + C \cdot \displaystyle\sum\limits_{i=1}^{n} \nabla l_i(\tilde{w}). \]

\subsubsection{Premiers Tests}
\label{donnees}
Afin de tester "visuellement" notre implémentation\footnote{\texttt{sgd.py}} en Python, nous avons écrit un générateur de données en dimension 2 qui fabrique des points répartis selon deux lois de moyennes différentes $\mu_1$ et $\mu_2$ uniformes respectivement sur $\lbrack \mu_1 - 1, \mu_1 + 1\rbrack$ et $\lbrack \mu_2 - 1, \mu_2 + 1\rbrack$.
Nous avons ensuite écrit une fonction qui affiche les points ainsi que la solution trouvée par la descente stochastique.

\begin{figure}[!h]
\includegraphics[width=0.85\textwidth]{../SGD}
\end{figure}

\subsubsection{Limites}
Cet algorithme permet de déterminer un hyperplan optimal pour une mesure donnée qui dépend notamment du coût attribué aux mauvaises classifications sur les données d'aprentissage. En revanche, il est difficilement adaptable à l'utilisation de noyau comme nous souhaitons le faire ici donc nous sommes ensuite passé à un second algorithme présenté dans le cours : le kernel perceptron.

\section{Kernel Perceptron}
\label{kernelperceptron}
En vue de tester différents noyaux mentionnés par l'article, nous avons implémenté l'algorithme du Kernel Perceptron\footnote{\texttt{kernel\_perceptron.py}}. Contrairement à l'algorithme de descente du gradient, cet algorithme se contente de trouver un hyperplan séparateur (dans le cas où il existe) sans optimiser de mesure sur cet hyperplan.

\subsection{Premiers tests}
Comme pour la SGD, nous avons testé le Kernel Perceptron sur des données générées avec le noyau trivial qui correspond à rester dans l'espace ambiant avec son produit scalaire) en dimension 2. Les résultats sont présentés sur la figure \ref{test_kp}. Les droites ont évolué dans le sens des aiguilles d'une montre au cours des itérations. La dernière droite sépare bien les données.

\begin{figure}[!h]
\includegraphics[width=0.85\textwidth]{img/kp_pdf}
\caption{Evolution de la droite testée par le kernel perceptron}\label{test_kp}
\end{figure}

\subsubsection{Limites}
Une difficulté qui se présente est le comportement de l'algorithme lorsque les données d'apprentissage ne sont pas linéairement séparable. Or rien à priori ne nous assure que le noyau de Fisher transforme nos données en des données séparables. Expérimentalement, la méthode suivante nous dit tout de même que les données sont séparables dans la plupart des cas. Pourtant, dans sa version actuelle, l'algorithme a régulièrement du mal à converger ce qui rend difficile son évaluation ainsi que l'optimisation des paramètres.

\section{Optimisation quadratique}
\label{optquad}
Pour tester une méthode alternative au kernel perceptron, nous avons utilisé une librairie matlab qui résout les problèmes d'optimisation quadratique : monqp.m écrite par \href{http://asi.insa-rouen.fr/enseignants/~scanu/SVM/}{Stéphane Canu} du LITIS de Rouen (cf. \cite{Canu}).
Les paramètres à ajuster de cet algorithm sont le coût attribué aux mauvaises classifications sur les données d'apprentissage et le nombre de gaussiennes lors de l'approximation de la fonction de répartition des données. Les fichiers de cette implémentation sont main\_qp.py, quad\_prog.py et qp.m.
Le principe de l'optimisation quadratique est de résoudre le problème dans le dual. Le problème initial \ref{eq:primalSVM} devient alors:

\begin{equation}
  \label{eq:dualSVM}
  \left\{
    \begin{array}{lll}
      \displaystyle\min_{\alpha}& \frac{1}{2} \alpha^\top  G \alpha - e^\top  \alpha&\\
      \mbox{avec} &y^\top  \alpha = 0 &\\
      \mbox{et} &0 \leq \alpha_i \leq C,& i=1 \dots n \\
    \end{array}
  \right.
\end{equation}

où $G$ est la matrice de terme général $G_{ij} =  y_i y_j x_i^\top x_j$,

%\section{Noyau Gaussien}
%\label{gaussien}
\section{Noyau de Fisher}
\label{fisher}
\subsection{GMM}
\label{gmm}
Le noyau de Fisher est construit à partir de modèles générateurs, dans notre cas, un GMM\footnote{Gaussian Mixture Model}.
Celui-ci produit à partir des données d'entraînement des paramètres $\theta_0 = \lbrace \mu_m^0, \sigma_m, \pi_m \rbrace_{m=1}^M$ décrivant $M$ gaussiennes\footnote{C'est-à-dire plus précisément de vecteurs gaussiens, dans le cas général où les données sont en dimension $k$.} de moyenne $\mu_m$ et de variance $\sigma_m$, et qui sont une approximation de la distribution des données.

Dans notre cas, on réalise un GMM pour un $M$ fixé (à partir de tests empiriques) sur des paquets de $T$ \og frames \fg \footnote{On rappelle qu'une frame représente une fenêtre de 20ms.}.

Pour cela, nous avons d'abord utilisé la librairie \texttt{gaussmix} de Voicebox avant de passer à la fonction mixture de \texttt{Scikit-Learn} pour Python à cause de problèmes de terminaison.
\subsection{Noyau de Fisher}
Étant donné un échantillon $\bar{x}$, le vecteur des scores de Fisher est:
\[u_{\bar{x}} = \nabla_{\theta} \log p (\bar{x} | \theta) \]

Alors le Kernel de Fisher est défini par:
\[K(\bar{x}_i,\bar{x}_j) = u_{\bar{x}_i}^{T} I(\theta)^{-1} u_{\bar{x}_j}\]
où $I(\theta)$ est la matrice d'information de Fisher, dont l'article affirme qu'on peut sans grande perte l'approximer par l'identité pour cette application. Nous ferons donc cette hypothèse et ne parlerons plus de cette matrice.

%(EXPLICATIONS INTERMEDIAIRES)

Finalement, on a:
\[ K(\bar{x}_i,\bar{x}_j) = \sum_{m=1}^M (n_{i,m} \Sigma_m^{-2} (\mu_m^i - \mu_m^0))^{T} (n_{j,m} \Sigma_m^{-2} (\mu_m^j - \mu_m^0)) \]
avec
\[n_{i,m} = \sum_{t=1}^{T} P(m | x_i^t),\]
$x_i^t$ étant la $t$-ième frame de $x_i$, $P(m | x_i^t)$ mesurant le taux d'appartenance de $x_i^t$ à la $m$-ième gaussienne, donnée qui est fournie par le GMM.


\section{Validation}
\label{validation}
\subsection{Validation croisée}
Pour choisir les paramètres à ajuster dans les différents algorithmes, nous avons utilisé le protocole suivant décrit dans l'article : nous avons séparé nos données en trois sous-ensembles. Un sous-ensemble d'apprentissage, un sous-ensemble de test pour l'optimisation des paramètres et un sous-ensemble d'évaluation pour ensuite valider les paramètres choisis.

\paragraph{SGD} Plus précisément, en ce qui concerne l'utilisation de la descente de gradient, le protocole est le suivant :
(il faut déteriner la constante $C$ la plus adaptée (\ref{sgd}))
\begin{itemize}
\item On choisit un sous-ensemble $S$ des locuteurs enregistrés.
\item Pour chaque locuteur, on partage les données selon les ratios suivants:
\begin{itemize}
\item $r_1$ vont dans l'ensemble d'entraînement;
\item $r_2$ vont dans l'ensemble de validation;
\item $1 - r_1 - r_2$ vont dans l'ensemble de \og post-validation \fg, accompagnés des locuteurs que l'on avait exclus à la première étape.
\end{itemize}
\item Pour chaque $C$ dans $2^{-k}, \dotsi, 1, \dotsi 2^k$, on calcule un $w$ correspondant à la valeur de $C$, et on calcule le taux de réussite sur l'ensemble de validation.
\item On en déduit une meilleure valeur de $C$, que l'on teste sur le troisième ensemble de \og post-validation \fg. Cela permet entre autres de voir si le résultat trouvé résiste aux \og attaques \fg, c'est-à-dire des locuteurs qui n'ont jamais été vus ni traités.
\end{itemize}

\begin{figure}[!h]
\includegraphics[width=0.85\textwidth]{../bestC}
\end{figure}
Cela donne par exemple la courbe ci-dessus, pour $k=4$ et les autres paramètres donnés sur le graphique:

On obtient $75.99 \%$ de réussite sur le troisième ensemble qui contient, en plus de données des locuteurs déjà entraînés, deux \og attaquants \fg. Ici, la meilleure valeur de $C$ est $2^1 = 2$.

\paragraph{Généralisation}

Finalement, il reste à mettre bout à bout ce qui vient d'être fait pour obtenir un programme qui construit le classifieur à partir des données, en passant par l'étape de validation croisée. L'ensemble est synthétisé sur le schema suivant (figure \ref{cross_validation}) et nous avons obtenu un programme qui génère un classifieur (basé sur SGD) à partir des données d'apprentissage, de la personne apprise et d'une plage de valeurs pour l'optimisation du paramètre C. Ce classifieur est en fait une fonction qui prend en entrée un fichier wav et qui retourne 0 ou 1 suivant la classification.


\begin{figure}
\begin{tikzpicture}[->,>=stealth']

 % Position of QUERY 
 % Use previously defined 'state' as layout (see above)
 % use tabular for content to get columns/rows
 % parbox to limit width of the listing
 \node[state] (QUERY) 
 {\begin{tabular}{l}
  \\
  \textbf{Dataset}\\
  \\[0.6em]

 \end{tabular}};
  
 % State: ACK with different content
 \node[state,    	% layout (defined above)
  text width=3cm, 	% max text width
  yshift=2cm, 		% move 2cm in y
  right of=QUERY, 	% Position is to the right of QUERY
  node distance=4.5cm, 	% distance to QUERY
  anchor=center] (ACK) 	% posistion relative to the center of the 'box'
 {%
 \begin{tabular}{l} 	% content
  \textbf{Evaluation Set}\\
  \parbox{2.8cm}{}
 \end{tabular}
 };
 
 % STATE QUERYREP
 \node[state,
  below of=ACK,
  yshift=-2cm,
  anchor=center,
  text width=3cm] (QUERYREP) 
 {%
 \begin{tabular}{l}
  \textbf{Validation}\\
  \parbox{2.8cm}{$2/3$ learning param. $C$\\
  $1/3$ evaluating $C$}
 \end{tabular}
 };

 % STATE EPC
 \node[state,
  right of=ACK,
  node distance=5cm,
  anchor=center] (EPC) 
 {%
 \begin{tabular}{l}
  \textbf{Final Classifier}\\
  \parbox{2cm}{+ accuracy}
 \end{tabular}
 };

 % draw the paths and and print some Text below/above the graph
 \path (QUERY) 	edge[bend left=15]  node[anchor=south,above]{split $1/3$} (ACK)
 (QUERY)     	edge[bend right=10] node[anchor=south,below]{split $2/3$} (QUERYREP)
 (ACK)       	edge                node[anchor=south,above]{computes}
 									node[anchor=south,below]{accuracy} (EPC)
 (QUERYREP)     edge                node[anchor=left,above]{Final} 
 									node[anchor=north,below]{Classifier}          (EPC)   
 (QUERYREP)  	edge[loop below]    node[anchor=north,below]{best $C$} (QUERYREP)
 (QUERYREP)  	edge                node[anchor=left,left]{Classifier with}
 	                                node[anchor=left,right]{best $C$} (ACK);


\end{tikzpicture}
\caption{How we compute the classifier for SGD}\label{cross_validation}
\end{figure}


\paragraph{Optimisation quadratique}
A priori, en ce qui concerne l'optimisation quadratique avec le noyau de Fisher, les paramètres à optimiser pour cet méthode sont le nombre de gaussiennes et la constante $C$.
Nous avons lancé des tests sur plusieurs valeurs des paramètres à ajuster. Nous avons observé que le paramètre $C$ n'influençait pas les résulats donc nous montrons seulement l'influence du nombre de gaussiennes sur la figure \ref{opt_qp}. Le nombre de gaussiennes optimal est autour de 30 et nous prendrons donc cette valeur par la suite, pour construire notre classifieur final.

\begin{figure}[!h]
\includegraphics[width=0.85\textwidth]{img/opt_qp_pdf}
\caption{Variation de la précision en fonction du nombre de gausiennes pour 4 interlocuteurs différents}\label{opt_qp}
\end{figure}

\section{Résultats}
\subsection{Imitateur}
\paragraph{SGD}
Avec l'algorithme sgd, en fixant le paramètre $C$ à $2$ et en apprenant sur 40\% des données, le taux d'échec est de $0.08$ et $0.01$ pour chacun de nous contre la personnalité politique alors que l'imitateur obtient un taux d'echec de $0.26$ : il est donc plus proche qu'une voix normale moyenne de la personnalité politique.
\label{imitateur}

\bibliographystyle{plain}
\bibliography{rapport}

\end{document}
